import math
import torch
import torch.nn as nn

"""The FeedForward class implements a fully connected neural network that applies independently and identically
to each position (token) in a sequence. It is an essential component in Transformers, used to process the representations
generated by the attention mechanism."""
class MultiHeadAttention(nn.Module):

    def __init__(self, embedding_dimension, number_heads):
        super(MultiHeadAttention, self).__init__()

        assert (embedding_dimension % number_heads == 0), "embedding_dimension must be divisible by number_heads"

        self.embedding_dimension = embedding_dimension
        self.number_heads = number_heads
        self.d_k = embedding_dimension // number_heads
        
        """These operations apply linear transformations to the input tensors to obtain the representations querys, key
        and values specific to the attention mechanism.
        * self.query_layer: Projects the entries into the query space, which represent "what" we are looking for in the context.
        * self.key_layer: Projects the entries into the key space, which represents "where" to look for relevant information.
        * self.value_layer: Projects the entries into the values space, which contain the information that will be extracted based on the queries and keys.
        These transformations allow the model to learn specific relationships between inputs in different subspaces"""
        self.query_layer = nn.Linear(embedding_dimension, embedding_dimension)
        self.key_layer = nn.Linear(embedding_dimension, embedding_dimension)
        self.value_layer = nn.Linear(embedding_dimension, embedding_dimension)
        self.output_layer = nn.Linear(embedding_dimension, embedding_dimension)

    def scaled_dot_product_attention(self, querys, key, values, mask=None):
        
        """These transformations allow the model to learn specific relationships between inputs in different subspaces.
        dividing it by the square root of the dimension of the querys vector and the key vector to reduce the magnitude
        and increase learning stability"""
        attention_scores = torch.matmul(querys, key.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        """It serves to mask certain positions in the attention_scores matrix. This ensures that the model
        ignore or penalize certain positions during attention calculation.
        For example, in text processing tasks, positions that correspond to filler tokens (<pad>) are often masked"""
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)

        attention_probabilities = torch.softmax(attention_scores, dim=-1)
        output = torch.matmul(attention_probabilities, values)

        return output

    """Split the tensor into multiple heads to:
    * Process smaller representation subspaces.
    * It allows the multi-head attention mechanism to operate efficiently.
    * Facilitate parallel computation on different heads, improving the model's ability to capture diverse patterns."""
    def split_heads(self, x):
        batch_size, seq_length, _ = x.size()

        return x.view(batch_size, seq_length, self.number_heads, self.d_k).transpose(1, 2)

    """The combine_heads function is the reverse operation of split_heads in the multi-head attention mechanism. Its purpose
    is to recombine the results of the multiple attention heads into a single output tensor that has the same original dimension.
    This is done after each head has calculated its attention independently"""
    def combine_heads(self, x):
        batch_size, _, seq_length, _ = x.size()

        return (x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embedding_dimension))

    def forward(self, querys, keys, values, mask=None):

        """The queries are divided into "Heads". Each head operates independently on a subspace of the representation. This allows
        calculations to be carried out in parallel, improving efficiency and capturing diverse relationships as each head can
        focus on different aspects of the information, as local or global relationships in the sequence."""
        querys = self.split_heads(self.query_layer(querys))
        keys = self.split_heads(self.key_layer(keys))
        values = self.split_heads(self.value_layer(values))

        """It calculates how relevant each element of a sequence (represented by the Keys and Values tensors) is in relation to a query
        specific (Querys). This calculation is done through a dot product between Querys and Keys, followed by scaling and
        normalization (softmax)."""
        attention_output = self.scaled_dot_product_attention(querys, keys, values, mask)

        """Each head captures different contextual patterns or relationships (such as local or global dependencies). By combining the heads,
        the model brings all this information together into a single tensor, which contains a more complete and enriched summary of the
        relationships within to sequence."""
        output = self.output_layer(self.combine_heads(attention_output))

        return output